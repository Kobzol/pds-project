\documentclass[a4paper,12pt,titlepage,oneside]{article}     

\usepackage{t1enc}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{biblatex}

\addbibresource{references.bib}

\title{Shortest path finding in undirected graphs using CUDA}
\author{Jakub Beránek}
\date{2016}

\newcommand{\parspace}[1][]{
	\ifthenelse{\isempty{#1}}{\vspace{5mm}}{\vspace{#1}}
	\par
}

\begin{document}
\maketitle

\begin{section}{Abstract}
Graphs have many practical applications, they can model road and communication networks, social interactions, flow of computation or data and many
other things. In a lot of these applications, it is often necessary to tell whether two vertices are connected and what is the distance between them.
The vertices and edges in those graphs are often counted in millions and serial processing of path queries in such large graphs can be slow. This paper focuses
on parallelizing graph path queries on Graphics Processing Units (GPUs) using CUDA, a framework for general purpose calculations on Nvidia graphic cards.
Shortest path querying is inherently a very iterative process, which means that it does not map straightforwardly to a massively parallel GPU
computation. The given mplementation of parallel path querying explores ways of overcoming this limitation by using modern CUDA features such as unified memory
addressing and atomic operations.
\end{section}

\begin{section}{Introduction}
Graphs are very useful in many areas of computer science, mathematics and other fields. They provide an effective and intuitive modeling tool that can be
easily applied to many real-world problems. With the advent of internet, vast amount of data is being collected and stored in various forms, graph storage
being one of them. Graphs of social network user interactions, customer behavior, articles or on-line videos are used virtually everywhere and they contain
massive amounts of data. Searching and querying this data thus takes a long time and it may be benefical to experiment with other architectures than the
classic processor for this task, for example graphic cards.

\parspace Graphics processing units are no longer used solely for graphics rendering and image manipulation. Several easy to grasp runtimes and libraries
(such as CUDA and OpenCL) allow developers to harness the raw power of thousands of GPU cores for many interesting applications including e.g. machine learning,
bio-inspired algorithms, hash cracking and other tasks that can benefit from a massively parallel approach.
This paper explores the possibility of accelerating selected graph search algorithms on GPUs using CUDA.
\end{section}

\begin{section}{State of the art}

\end{section}

\begin{section}{CUDA overview}
CUDA is a development platform and a programming interface (API) created by Nvidia that allows programmers to execute (almost) arbitrary code directly on the
GPU. This simplifies the development process for general computing on graphic cards, because without it developers had to model all problems and algorithms
in explicit graphic resources such as vertices and textures. CUDA has been released in 2007 and since then it had several revisions with new features added, the newest version is 7.5. It supports C, C++ and Fortran (the main languages used in scientific computing), but it has bindings for many more languages.
Code that will run on GPU can be written in plain C or C++ with only a few changes. CUDA provides several functions, variables and macros that mark the part
of the code that will be executed by the GPU. This code is then compiled by the CUDA compiler NVCC and linked to the rest of the code that is run on CPU.

\parspace Each CUDA-enabled device has a global device memory and several multiprocessors, each with it's own shared memory, registers and processors.
The multiprocessors execute functions (called \textit{kernels}) in warps, groups of 32 threads. Because the threads are being executed in groups and they
share a lot of resources (memory and registers), it is beneficial to execute the same path for every thread. That means that for optimal performance,
conditions in code should be minimized. The memory of the GPU and CPU is not shared, so any memory movements have to be done explicitly by the programmer.
Typical CUDA computation first transfers data from CPU to GPU memory, than starts parallel computation on the GPU and when the computation is finished,
the results are gathered back to the CPU. The memory transfer from CPU to GPU and vice versa is a slow operation and it should be ideally avoided as
much as possible. The gap between CPU and GPU memory is somewhat alleviated in newer CUDA versions that provide unified memory. With this function, the CPU
can map a portion of it's address space and provide direct access for the GPU to access this area of memory. The newest CUDA architecture, Pascal, will even
allow to intertwine the address spaces of the processor and the graphic device to create one large address space that is accessible by both of them. That will
allow to further simplify the development process, because the GPU will be able to access memory allocated on the processor without requiring to use any special
allocation functions\cite{CUDAPascal}.
\end{section}

\begin{section}{Algorithm overview}
\end{section}

\begin{section}{Experimental results}
\end{section}

\begin{section}{Conclusion}
\end{section}

\printbibliography

\end{document}



